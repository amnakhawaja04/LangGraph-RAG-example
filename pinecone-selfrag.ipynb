{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f513e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pinecone\n",
      "  Downloading pinecone-8.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone) (2025.11.12)\n",
      "Requirement already satisfied: orjson>=3.0.0 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone) (3.11.5)\n",
      "Collecting pinecone-plugin-assistant<4.0.0,>=3.0.1 (from pinecone)\n",
      "  Downloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.1.0,>=0.0.7 in c:\\users\\pmls\\appdata\\roaming\\python\\python312\\site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone) (4.15.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone) (2.3.0)\n",
      "Collecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
      "Downloading pinecone-8.0.0-py3-none-any.whl (745 kB)\n",
      "   ---------------------------------------- 0.0/745.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/745.9 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/745.9 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/745.9 kB ? eta -:--:--\n",
      "   -------------------------- ----------- 524.3/745.9 kB 699.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 745.9/745.9 kB 774.0 kB/s  0:00:01\n",
      "Downloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl (280 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: packaging, pinecone-plugin-assistant, pinecone\n",
      "\n",
      "   ---------------------------------------- 0/3 [packaging]\n",
      "   ---------------------------------------- 0/3 [packaging]\n",
      "   ---------------------------------------- 0/3 [packaging]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   ------------- -------------------------- 1/3 [pinecone-plugin-assistant]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   -------------------------- ------------- 2/3 [pinecone]\n",
      "   ---------------------------------------- 3/3 [pinecone]\n",
      "\n",
      "Successfully installed packaging-24.2 pinecone-8.0.0 pinecone-plugin-assistant-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.14 requires protobuf<5,>=4.25.3, but you have protobuf 6.33.2 which is incompatible.\n",
      "streamlit 1.35.0 requires protobuf<5,>=3.20, but you have protobuf 6.33.2 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires ml-dtypes~=0.3.1, but you have ml-dtypes 0.5.4 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.20.0 which is incompatible.\n",
      "tensorflow-serving-api 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214099a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# add pinecone and hf api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92593bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PMLS\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore   # âœ… FIX\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec18b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths & models\n",
    "DATA_DIR = r\"C:\\Users\\PMLS\\Downloads\\langgraph\\lmkr_data\"\n",
    "INDEX_NAME = \"lmkr-self-rag\"\n",
    "\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# LLM_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b723aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_chat(messages, max_tokens=512, temperature=0.0):\n",
    "    client = InferenceClient(\n",
    "        model=LLM_MODEL,\n",
    "        token=os.getenv(\"HF_TOKEN\"),\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    start, end = text.find(\"{\"), text.rfind(\"}\")\n",
    "    return json.loads(text[start : end + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b89f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=384,  # all-MiniLM-L6-v2\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=os.getenv(\"PINECONE_ENVIRONMENT\"),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244b987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "def ingest_documents():\n",
    "    docs = []\n",
    "    for f in os.listdir(DATA_DIR):\n",
    "        if f.endswith(\".txt\"):\n",
    "            docs.extend(\n",
    "                TextLoader(\n",
    "                    os.path.join(DATA_DIR, f),\n",
    "                    encoding=\"utf-8\",\n",
    "                ).load()\n",
    "            )\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=50,\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    PineconeVectorStore.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9efb576",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(INDEX_NAME)\n",
    "if index.describe_index_stats()[\"total_vector_count\"] == 0:\n",
    "    ingest_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc46ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "035290b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_binary_grade(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Robustly extract yes/no from HF model output.\n",
    "    Defaults to 'no' if uncertain (conservative).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = extract_json(text)\n",
    "    except Exception:\n",
    "        return \"no\"\n",
    "\n",
    "    for key in [\"binary_score\", \"score\", \"answer\", \"label\"]:\n",
    "        if key in data:\n",
    "            val = str(data[key]).strip().lower()\n",
    "            if val in [\"yes\", \"no\"]:\n",
    "                return val\n",
    "\n",
    "    return \"no\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b688e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryGrade(BaseModel):\n",
    "    binary_score: Literal[\"yes\", \"no\"]\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    generation: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4762af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: GraphState):\n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    return {\n",
    "        \"question\": state[\"question\"],\n",
    "        \"documents\": [d.page_content for d in docs],\n",
    "        \"generation\": \"\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a388ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_docs(state: GraphState):\n",
    "    filtered = []\n",
    "\n",
    "    for d in state[\"documents\"]:\n",
    "        result = hf_chat([\n",
    "            {\"role\": \"system\", \"content\": \"Return ONLY JSON.\"},\n",
    "            {\"role\": \"user\", \"content\":\n",
    "                f\"Question:\\n{state['question']}\\n\\n\"\n",
    "                f\"Document:\\n{d}\\n\\n\"\n",
    "                \"Is it relevant? Respond as JSON with yes/no.\"\n",
    "            },\n",
    "        ])\n",
    "\n",
    "        if safe_binary_grade(result) == \"yes\":\n",
    "            filtered.append(d)\n",
    "\n",
    "    return {**state, \"documents\": filtered}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb48159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: GraphState):\n",
    "    answer = hf_chat([\n",
    "        {\"role\": \"system\", \"content\": \"Use ONLY the provided context.\"},\n",
    "        {\"role\": \"user\", \"content\":\n",
    "            f\"Context:\\n{state['documents']}\\n\\n\"\n",
    "            f\"Question:\\n{state['question']}\"\n",
    "        },\n",
    "    ], temperature=0.2)\n",
    "\n",
    "    return {**state, \"generation\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db9fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: GraphState):\n",
    "    new_q = hf_chat([\n",
    "        {\"role\": \"system\", \"content\": \"Rewrite the question to improve retrieval.\"},\n",
    "        {\"role\": \"user\", \"content\": state[\"question\"]},\n",
    "    ])\n",
    "    return {\"question\": new_q, \"documents\": [], \"generation\": \"\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6eca4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_docs(state):\n",
    "    return \"rewrite\" if len(state[\"documents\"]) == 0 else \"generate\"\n",
    "\n",
    "\n",
    "def grade_answer(state):\n",
    "    result = hf_chat([\n",
    "        {\"role\": \"system\", \"content\": \"Return ONLY JSON.\"},\n",
    "        {\"role\": \"user\", \"content\":\n",
    "            f\"Documents:\\n{state['documents']}\\n\\n\"\n",
    "            f\"Answer:\\n{state['generation']}\\n\\n\"\n",
    "            \"Is the answer grounded? {\\\"binary_score\\\":\\\"yes\\\"|\\\"no\\\"}\"\n",
    "        },\n",
    "    ])\n",
    "    return END if safe_binary_grade(result) == \"yes\" else \"rewrite\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e19dca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_docs\", grade_docs)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"rewrite\", rewrite_query)\n",
    "\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_docs\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_docs\",\n",
    "    decide_docs,\n",
    "    {\"rewrite\": \"rewrite\", \"generate\": \"generate\"},\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_answer,\n",
    "    {END: END, \"rewrite\": \"rewrite\"},\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"rewrite\", \"retrieve\")\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "344bfdd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/featherless-ai/v1/chat/completions (Request ID: Root=1-69401a15-5696b0323137c1d438a0c436;483a5384-4a80-4dda-b3b7-3474002e24b1)\n\nYou have reached the free monthly usage limit for featherless-ai. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/featherless-ai/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat is lmkr?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL ANSWER:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mgrade_docs\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      2\u001b[39m filtered = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m state[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     result = \u001b[43mhf_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReturn ONLY JSON.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQuestion:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDocument:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43md\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIs it relevant? Respond as JSON with yes/no.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m safe_binary_grade(result) == \u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     15\u001b[39m         filtered.append(d)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mhf_chat\u001b[39m\u001b[34m(messages, max_tokens, temperature)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhf_chat\u001b[39m(messages, max_tokens=\u001b[32m512\u001b[39m, temperature=\u001b[32m0.0\u001b[39m):\n\u001b[32m      2\u001b[39m     client = InferenceClient(\n\u001b[32m      3\u001b[39m         model=LLM_MODEL,\n\u001b[32m      4\u001b[39m         token=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:915\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    887\u001b[39m parameters = {\n\u001b[32m    888\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    889\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    906\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    907\u001b[39m }\n\u001b[32m    908\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    909\u001b[39m     inputs=messages,\n\u001b[32m    910\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    914\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:275\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PMLS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:475\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/featherless-ai/v1/chat/completions (Request ID: Root=1-69401a15-5696b0323137c1d438a0c436;483a5384-4a80-4dda-b3b7-3474002e24b1)\n\nYou have reached the free monthly usage limit for featherless-ai. Subscribe to PRO to get 20x more included usage, or add pre-paid credits to your account.",
      "During task with name 'grade_docs' and id '172d98bb-8dd7-dde4-db82-71e4224f98ba'"
     ]
    }
   ],
   "source": [
    "question = \"What is lmkr?\"\n",
    "result = app.invoke({\"question\": question})\n",
    "\n",
    "print(\"FINAL ANSWER:\\n\")\n",
    "print(result[\"generation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95f396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
